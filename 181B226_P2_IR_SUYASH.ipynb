{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P2\n",
    "# SUYASH PRATAP SINGH 181B226\n",
    "# Set-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASKS:-\n",
    "\n",
    "Consider the Information retrieval system created by you as Doodle.com. Below is the list of documents retrieved by your search engine in order. The relevance ranking of any result to a query is measured on the basis of number of words matching in the document to the query. The documents having highest number of matching words will be considered most relevant. Complete the following tasks, and submit the pdf file of your notebook with outputs.\n",
    "\n",
    "Calculate and show the ranking of each document.\n",
    "Calculate the value of recall and precision at every level of retrieved document.\n",
    "Calculate the Average precision.\n",
    "Compare the performance of your system with XYZ.com. the query: \" Human-based review of computer systems\" Doodle.com Result (In order) c1: Human machine interface for, Lab ABC computer applications c2: A survey of user opinion. of computer system response time c3: The EPS user interface, management system c4: System and human system, engineering testing of EPS c5: Relation of user-perceived response time to error measurement m1: The generation of random, binary, unordered trees m2: The intersection graph, of paths in trees m3: Graph minors IV: Widths of trees and well-quasi-ordering m4: Graph minors: A survey XYZ.com (In order) m1: The generation of random, binary, unordered trees m2: The intersection graph, of paths in trees m3: Graph minors IV: Widths of trees and well-quasi-ordering m4: Graph minors: A survey c1: Human machine interface for, Lab ABC computer applications c2: A survey of user opinion. of computer system response time c3: The EPS user interface, management system c4: System and human system, engineering testing of EPS c5: Relation of user-perceived response time to error measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.feature_extraction\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doodle=['Human machine interface for, Lab ABC computer applications',\n",
    "        'A survey of user opinion. of computer system response time',\n",
    "        'The EPS user interface, management system',\n",
    "        'System and human system, engineering testing of EPS',\n",
    "        'Relation of user-perceived response time to error measurement',\n",
    "        'The generation of random, binary, unordered trees',\n",
    "        'The intersection graph, of paths in trees',\n",
    "        'Graph minors IV: Widths of trees and well-quasi-ordering',\n",
    "        'Graph minors: A survey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz=['The generation of random, binary, unordered trees',\n",
    "     'The intersection graph, of paths in trees',\n",
    "     'Graph minors IV: Widths of trees and well-quasi-ordering',\n",
    "     'Graph minors: A survey',\n",
    "     'Human machine interface for, Lab ABC computer applications',\n",
    "     'A survey of user opinion. of computer system response time',\n",
    "     'The EPS user interface, management system',\n",
    "     'System and human system, engineering testing of EPS',\n",
    "     'Relation of user-perceived response time to error measurement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='Human- based review of computer systems'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Calculate and show the ranking of each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doodle Rankings: [2, 2, 0, 2, 1, 1, 1, 1, 0]\n",
      "XYZ Rankings:  [1, 1, 1, 0, 2, 2, 0, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "def rank(doc,query):\n",
    "    rankings=[]\n",
    "    for i in doc:\n",
    "        count=0\n",
    "        for j in query.split():\n",
    "            if j in i:\n",
    "                count+=1\n",
    "        rankings.append(count)\n",
    "    return rankings\n",
    "\n",
    "doodle_rankings=rank(doodle,query)\n",
    "print('Doodle Rankings:', doodle_rankings)\n",
    "xyz_rankings=rank(xyz,query)\n",
    "print('XYZ Rankings: ',xyz_rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Calculate the value of recall and precision at every level of retrieved document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 1, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=len(doodle_rankings)\n",
    "for i in range(d):\n",
    "    if doodle_rankings[i]>0:\n",
    "        doodle_rankings[i]=1\n",
    "doodle_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 0, 1, 1, 0, 1, 1]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(d):\n",
    "    if xyz_rankings[i]>0:\n",
    "        xyz_rankings[i]=1\n",
    "xyz_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall and Precision at each step\n",
      "1 \t0.143 \t1.000\n",
      "2 \t0.286 \t1.000\n",
      "3 \t0.286 \t0.667\n",
      "4 \t0.429 \t0.750\n",
      "5 \t0.571 \t0.800\n",
      "6 \t0.714 \t0.833\n",
      "7 \t0.857 \t0.857\n",
      "8 \t1.000 \t0.875\n",
      "9 \t1.000 \t0.778\n"
     ]
    }
   ],
   "source": [
    "total_relevant=sum(doodle_rankings)\n",
    "precision=[0]*d\n",
    "recall=[0]*d\n",
    "for i in range(d):\n",
    "    precision[i]=sum(doodle_rankings[:i+1])/len(doodle_rankings[:i+1])\n",
    "    recall[i]=sum(doodle_rankings[:i+1])/total_relevant\n",
    "print('Recall and Precision at each step')\n",
    "for i in range(d):\n",
    "    print(i+1,'\\t%.3f'%recall[i],'\\t%.3f'%precision[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision [1.0, 1.0, 0.6666666666666666, 0.75, 0.8, 0.8333333333333334, 0.8571428571428571, 0.875, 0.7777777777777778]\n",
      "Recall [0.14285714285714285, 0.2857142857142857, 0.2857142857142857, 0.42857142857142855, 0.5714285714285714, 0.7142857142857143, 0.8571428571428571, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "relevance_order=[1,1,0,1,1,1,1,1,0] #doodle.com\n",
    "relevant=0\n",
    "total=0\n",
    "t_relevant=7\n",
    "precision=[]\n",
    "recall=[]\n",
    "j=1\n",
    "for i in relevance_order:\n",
    "    if(i==1):\n",
    "        relevant+=1\n",
    "    precision.append(relevant/j)\n",
    "    recall.append(relevant/t_relevant)\n",
    "    j+=1\n",
    "print(\"Precision\",precision)\n",
    "print(\"Recall\",recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision [1.0, 1.0, 1.0, 0.75, 0.8, 0.8333333333333334, 0.7142857142857143, 0.75, 0.7777777777777778]\n",
      "Recall [0.14285714285714285, 0.2857142857142857, 0.42857142857142855, 0.42857142857142855, 0.5714285714285714, 0.7142857142857143, 0.7142857142857143, 0.8571428571428571, 1.0]\n"
     ]
    }
   ],
   "source": [
    "relevance_order=[1,1,1,0,1,1,0,1,1]  #xyz.com\n",
    "relevant=0\n",
    "total=0\n",
    "t_relevant=7\n",
    "precision=[]\n",
    "recall=[]\n",
    "j=1\n",
    "for i in relevance_order:\n",
    "    if(i==1):\n",
    "        relevant+=1\n",
    "    precision.append(relevant/j)\n",
    "    recall.append(relevant/t_relevant)\n",
    "    j+=1\n",
    "print(\"Precision\",precision)\n",
    "print(\"Recall\",recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. 3.\tCalculate the Average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ranking 1(precision) = 1+1+.75+.8+.833+.857+.875 = 6.105  #taking precision value of relevant document\n",
    "#Ranking 2(precision) = 1+1+1+.8+.83+.75+.77 = 6.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average precision 1 = Ranking1/7 = 6.105/7 = .8721\n",
    "#average precision 2 = Ranking2/7 = 6.15/7  = .8785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean average precision = #average precision 1+#average precision 2/2 = 1.7506/2 =.8753"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP for system 1 2.5095238095238095\n",
      "MAP for system 2 2.45\n"
     ]
    }
   ],
   "source": [
    "relevance_doc2_order=[1,1,1,0,1,1,1,0,1]\n",
    "relevance_doc1_order=[1,1,0,1,1,1,1,1,0]\n",
    "relevant1=0\n",
    "relevant2=0\n",
    "precision1=[]\n",
    "precision2=[]\n",
    "recall=[]\n",
    "j=1\n",
    "for i in range(8):\n",
    "    if(relevance_doc1_order[i]==1):\n",
    "        relevant1+=1\n",
    "        precision1.append(relevant/j)\n",
    "    if(relevance_doc2_order[i]==1):\n",
    "        relevant2+=1\n",
    "        precision2.append(relevant/j)\n",
    "        j+=1\n",
    "print(\"MAP for system 1\",sum(precision1)/7)\n",
    "print(\"MAP for system 2\",sum(precision2)/7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Compare the performance of your system with XYZ.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 0, 2, 2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_mat=[0]*4\n",
    "for i in range(n):\n",
    "    if doodle_rankings[i]==1 and xyz_rankings[i]==1:\n",
    "        compare_mat[0]+=1\n",
    "    elif doodle_rankings[i]==0 and xyz_rankings[i]==0:\n",
    "        compare_mat[1]+=1\n",
    "    elif doodle_rankings[i]==1 and xyz_rankings[i]==0:\n",
    "        compare_mat[2]+=1\n",
    "    else:\n",
    "        compare_mat[3]+=1\n",
    "compare_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_A=(compare_mat[0]+compare_mat[1])/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(non relevant)= 0.222\n",
      "P(relevant)= 0.778\n",
      "Kappa= -0.286\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in doodle_rankings:\n",
    "    if i==0:\n",
    "        count+=1\n",
    "for i in xyz_rankings:\n",
    "    if i==0:\n",
    "        count+=1\n",
    "P_NR=count/(2*n)\n",
    "P_R=1-P_NR\n",
    "print('P(non relevant)= %.3f'%P_NR)\n",
    "print('P(relevant)= %.3f'%P_R)\n",
    "P_E=P_NR**2+P_R**2\n",
    "kappa=(P_A-P_E)/(1-P_E)\n",
    "print('Kappa= %.3f'%kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=['Human machine interface for Lab ABC computer applications',\n",
    " 'A survey of user opinion of computer system response time',\n",
    " 'The EPS user interface management system',\n",
    " 'System and human system engineering testing of EPS',\n",
    " 'Relation of user-perceived response time to error measurement',\n",
    " 'The generation of random, binary, unordered trees',\n",
    " 'The intersection graph of paths in trees',\n",
    " 'Graph minors IV: Widths of trees and well-quasi-ordering',\n",
    " 'Graph minors: A survey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \" Human-based review of computer systems\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.20311632, 0.20527035, 0.        , 0.14940581,\n",
       "        0.04238244, 0.04931835, 0.05151946, 0.04084988, 0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer()\n",
    "doc_vectors = vect.fit_transform([query]+docs)\n",
    "cosine_sim = linear_kernel(doc_vectors[0:1], doc_vectors)\n",
    "cosine_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:- #For Doodle.com\n",
    "#Rank 1: c2 #Rank 2: c1 #Rank 3 : c4 #Rank 4 : m2 #Rank 5 : m1 #Rank 6 : c5 #Rank 7 : m3 #Rank 8 : c3,m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs1=['The generation of random, binary, unordered trees',\n",
    " 'The intersection graph of paths in trees',\n",
    " 'Graph minors IV: Widths of trees and well-quasi-ordering',\n",
    " 'Graph minors: A survey','Human machine interface for Lab ABC computer applications',\n",
    " 'A survey of user opinion of computer system response time',\n",
    " 'The EPS user interface management system',\n",
    " 'System and human system engineering testing of EPS',\n",
    " 'Relation of user-perceived response time to error measurement'] #xyz.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.04931835, 0.05151946, 0.04084988, 0.        ,\n",
       "        0.20311632, 0.20527035, 0.        , 0.14940581, 0.04238244]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer()\n",
    "doc_vectors = vect.fit_transform([query]+docs1)\n",
    "cosine_sim = linear_kernel(doc_vectors[0:1], doc_vectors)\n",
    "cosine_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:- RANKS for XYZ.COM:- C2>C1>C4>m2>m1>m3>m4=C3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THANK YOU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
