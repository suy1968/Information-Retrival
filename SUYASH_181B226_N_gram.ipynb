{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUYASH PRATAP SINGH(181B226)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASKS:- 1\n",
    "\n",
    "1. Implement the Levenshtein distance in python.\n",
    "2. Create a vocabulary of a minimum of 100 words.\n",
    "3. Take any sample word as input.\n",
    "4. The code should calculate the edit distance from every word in vocabulary and display only those words as output havin levenshtein distance of  less tahn or equal to 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def med(word1,word2):\n",
    "    word1,word2=word1.upper(),word2.upper()\n",
    "    #Initializing matrix\n",
    "    len1,len2 = len(word1),len(word2)\n",
    "    matrix = np.array([['0 ']*(len2+2)]*(len1+2))\n",
    "    for i in range(len1):\n",
    "        matrix[i+2][0],matrix[i+2][1] = word1[i],i+1\n",
    "    for i in range(len2):\n",
    "        matrix[0][i+2],matrix[1][i+2] = word2[i],i+1\n",
    "    #Calculating Minimum Edit Distance    \n",
    "    for i in range(2,len1+2):\n",
    "        for j in range(2,len2+2):\n",
    "            prev = min(matrix[i-1][j-1],matrix[i-1][j],matrix[i][j-1])\n",
    "            matrix[i][j] = int(prev)+(matrix[i][0] != matrix[0][j])\n",
    "    return int(matrix[-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med('asdf','ardg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(r'C:\\Users\\Admin\\Downloads\\Suyash.txt')\n",
    "mafia=file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuations\n",
    "import re \n",
    "mafia = [re.sub(r'[^\\w\\s]', '', x).lower() for x in mafia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in information theory linguistics and computer science the levenshtein distance is a string metric for measuring the difference between two sequences informally the levenshtein distance between two words is the minimum number of singlecharacter edits insertions deletions or substitutions required to change one word into the other it is named after the soviet mathematician vladimir levenshtein who considered this distance in 1965 levenshtein distance may also be referred to as edit distance although that term may also denote a larger family of distance metrics known collectively as edit distance232 it is closely related to pairwise string alignments in approximate string matching the objective is to find matches for short strings in many longer texts in situations where a small number of differences is to be expected the short strings could come from a dictionary for instance here one of the strings is typically short while the other is arbitrarily long this has a wide range of applications for instance spell checkers correction systems for optical character recognition and software to assist natural language translation based on translation memory the levenshtein distance can also be computed between two longer strings but the cost to compute it which is roughly proportional to the product of the two string lengths makes this impractical thus when used to aid in fuzzy string searching in applications such as record linkage the compared strings are usually short to help improve speed of comparisons in linguistics the levenshtein distance is used as a metric to quantify the linguistic distance or how different two languages are from one another3 it is related to mutual intelligibility the higher the linguistic distance the lower the mutual intelligibility and the lower the linguistic distance the higher the mutual intelligibility computing the levenshtein distance is based on the observation that if we reserve a matrix to hold the levenshtein distances between all prefixes of the first string and all prefixes of the second then we can compute the values in the matrix in a dynamic programming fashion and thus find the distance between the two full strings as the last value computed iterative with two matrix rows it turns out that only two rows of the table are needed for the construction if one does not want to reconstruct the edited input strings the previous row and the current row being calculated the levenshtein distance may be calculated iteratively using the following algorithm this two row variant is suboptimal√¢the amount of memory required may be reduced to one row and one index word of overhead for better cache locality hirschbergs algorithm combines this method with divide and conquer it can compute the optimal edit sequence and not just the edit distance in the same asymptotic time and space bounds the dynamic variant is not the ideal implementation an adaptive approach may reduce the amount of memory required and in the best case may reduce the time complexity to linear in the length of the shortest string and in the worst case no more than quadratic in the length of the shortest string the idea is that one can use efficient library functions to check for common prefixes and suffixes and only dive into the dp part on mismatch']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mafia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=set(mafia[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(word):\n",
    "    for i in tokens:\n",
    "        n = med(word,i)\n",
    "        if n < 3:\n",
    "            print(n,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def levenshteinDistanceDP(token1, token2):\n",
    "    distances = numpy.zeros((len(token1) + 1, len(token2) + 1))\n",
    "\n",
    "    for t1 in range(len(token1) + 1):\n",
    "        distances[t1][0] = t1\n",
    "\n",
    "    for t2 in range(len(token2) + 1):\n",
    "        distances[0][t2] = t2\n",
    "        \n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    \n",
    "    for t1 in range(1, len(token1) + 1):\n",
    "        for t2 in range(1, len(token2) + 1):\n",
    "            if (token1[t1-1] == token2[t2-1]):\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "                \n",
    "                if (a <= b and a <= c):\n",
    "                    distances[t1][t2] = a + 1\n",
    "                elif (b <= a and b <= c):\n",
    "                    distances[t1][t2] = b + 1\n",
    "                else:\n",
    "                    distances[t1][t2] = c + 1\n",
    "\n",
    "    \n",
    "    return distances[len(token1)][len(token2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "distance = levenshteinDistanceDP('asdf','ardg')\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "teja=[]\n",
    "while _ in range(100):\n",
    "    teja.append(input())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "teja=['comparison', 'bot', 'influence', 'exciting', 'outgoing', 'damage', 'mice', 'wretched', 'early', 'nonstop', 'song', \n",
    "   'aback', 'laborer', 'guard', 'aunt', 'lumber', 'grape', 'beautiful', 'passenger', 'ornament', 'barbarous', 'vein', \n",
    "   'claim', 'detailed', 'love', 'zipper', 'smoggy', 'language', 'protective', 'rifle', 'coast', 'guess', 'spiky',\n",
    "   'multiply', 'birth', 'railway', 'stiff', 'discover', 'book', 'fall', 'mountainous', 'assorted', 'lyrical',\n",
    "   'jittery', 'murky', 'smelly', 'ban', 'realize', 'rhetorical', 'efficient', 'prefer', 'cows', 'cheerful', \n",
    "   'toad', 'deafening', 'stop', 'invent', 'cook', 'interesting', 'useless', 'clear', 'jog', 'cheap', 'raise', \n",
    "   'avoid', 'increase', 'noisy', 'serious', 'sea', 'spiteful', 'loud', 'rod', 'replace', 'oranges','border', 'second', 'cactus',\n",
    "   'perpetual', 'grip', 'dispensable','sprout', 'pat', 'friend', 'boat', 'half', 'elbow', 'poor', 'beak', 'busy', 'monkey',\n",
    "   'control', 'cautious', 'trace', 'deletion', 'disarm', 'mundane', 'divergent', 'horn', 'cars', 'prose']\n",
    "print(len(teja))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(w1, w2, m, n):\n",
    "    if m == 0:\n",
    "        return n\n",
    "    if n == 0:\n",
    "        return m\n",
    "    if w1[m-1] == w2[n-1]:\n",
    "        return edit_distance(w1, w2, m-1, n-1)\n",
    "    return 1 + min(edit_distance(w1, w2, m, n-1),    # Insertion property follow\n",
    "                   edit_distance(w1, w2, m-1, n),    # deletion property follow\n",
    "                   edit_distance(w1, w2, m-1, n-1)    # substituion property follow\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "toad\n",
      "loud\n",
      "rod\n"
     ]
    }
   ],
   "source": [
    "s='loud'\n",
    "for i in teja:\n",
    "    dist=edit_distance(s, i, len(s), len(i))\n",
    "    if(dist<=2):\n",
    "        print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASKS:2\n",
    "1. Upgrade your last submitted assignment to support the n-gram overlap method.\n",
    "2. In the last assignment you checked the edit distance of the query word with every word in the vocabulary.\n",
    "3. Now you will compute the edit distance with only those words which have atleast 3 trigram overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "teja=['comparison', 'bot', 'influence', 'exciting', 'outgoing', 'damage', 'mice', 'wretched', 'early', 'nonstop', 'song', \n",
    "   'aback', 'laborer', 'guard', 'aunt', 'lumber', 'grape', 'beautiful', 'passenger', 'ornament', 'barbarous', 'vein', \n",
    "   'claim', 'detailed', 'love', 'zipper', 'smoggy', 'language', 'protective', 'rifle', 'coast', 'guess', 'spiky',\n",
    "   'multiply', 'birth', 'railway', 'stiff', 'discover', 'book', 'fall', 'mountainous', 'assorted', 'lyrical',\n",
    "   'jittery', 'murky', 'smelly', 'ban', 'realize', 'rhetorical', 'efficient', 'prefer', 'cows', 'cheerful', \n",
    "   'toad', 'deafening', 'stop', 'invent', 'cook', 'interesting', 'useless', 'clear', 'jog', 'cheap', 'raise', \n",
    "   'avoid', 'increase', 'noisy', 'serious', 'sea', 'spiteful', 'loud', 'rod', 'replace', 'oranges','border', 'second', 'cactus',\n",
    "   'perpetual', 'grip', 'dispensable','sprout', 'pat', 'friend', 'boat', 'half', 'elbow', 'poor', 'beak', 'busy', 'monkey',\n",
    "   'control', 'cautious', 'trace', 'deletion', 'disarm', 'november', 'december', 'horn', 'cars', 'prose']\n",
    "print(len(teja))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={}\n",
    "for i in teja:\n",
    "    trigrams=list(ngrams(i, 3))\n",
    "    d[i]=trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparison -> [('c', 'o', 'm'), ('o', 'm', 'p'), ('m', 'p', 'a'), ('p', 'a', 'r'), ('a', 'r', 'i'), ('r', 'i', 's'), ('i', 's', 'o'), ('s', 'o', 'n')]\n",
      "bot -> [('b', 'o', 't')]\n",
      "influence -> [('i', 'n', 'f'), ('n', 'f', 'l'), ('f', 'l', 'u'), ('l', 'u', 'e'), ('u', 'e', 'n'), ('e', 'n', 'c'), ('n', 'c', 'e')]\n",
      "exciting -> [('e', 'x', 'c'), ('x', 'c', 'i'), ('c', 'i', 't'), ('i', 't', 'i'), ('t', 'i', 'n'), ('i', 'n', 'g')]\n",
      "outgoing -> [('o', 'u', 't'), ('u', 't', 'g'), ('t', 'g', 'o'), ('g', 'o', 'i'), ('o', 'i', 'n'), ('i', 'n', 'g')]\n",
      "damage -> [('d', 'a', 'm'), ('a', 'm', 'a'), ('m', 'a', 'g'), ('a', 'g', 'e')]\n",
      "mice -> [('m', 'i', 'c'), ('i', 'c', 'e')]\n",
      "wretched -> [('w', 'r', 'e'), ('r', 'e', 't'), ('e', 't', 'c'), ('t', 'c', 'h'), ('c', 'h', 'e'), ('h', 'e', 'd')]\n",
      "early -> [('e', 'a', 'r'), ('a', 'r', 'l'), ('r', 'l', 'y')]\n",
      "nonstop -> [('n', 'o', 'n'), ('o', 'n', 's'), ('n', 's', 't'), ('s', 't', 'o'), ('t', 'o', 'p')]\n",
      "song -> [('s', 'o', 'n'), ('o', 'n', 'g')]\n",
      "aback -> [('a', 'b', 'a'), ('b', 'a', 'c'), ('a', 'c', 'k')]\n",
      "laborer -> [('l', 'a', 'b'), ('a', 'b', 'o'), ('b', 'o', 'r'), ('o', 'r', 'e'), ('r', 'e', 'r')]\n",
      "guard -> [('g', 'u', 'a'), ('u', 'a', 'r'), ('a', 'r', 'd')]\n",
      "aunt -> [('a', 'u', 'n'), ('u', 'n', 't')]\n",
      "lumber -> [('l', 'u', 'm'), ('u', 'm', 'b'), ('m', 'b', 'e'), ('b', 'e', 'r')]\n",
      "grape -> [('g', 'r', 'a'), ('r', 'a', 'p'), ('a', 'p', 'e')]\n",
      "beautiful -> [('b', 'e', 'a'), ('e', 'a', 'u'), ('a', 'u', 't'), ('u', 't', 'i'), ('t', 'i', 'f'), ('i', 'f', 'u'), ('f', 'u', 'l')]\n",
      "passenger -> [('p', 'a', 's'), ('a', 's', 's'), ('s', 's', 'e'), ('s', 'e', 'n'), ('e', 'n', 'g'), ('n', 'g', 'e'), ('g', 'e', 'r')]\n",
      "ornament -> [('o', 'r', 'n'), ('r', 'n', 'a'), ('n', 'a', 'm'), ('a', 'm', 'e'), ('m', 'e', 'n'), ('e', 'n', 't')]\n",
      "barbarous -> [('b', 'a', 'r'), ('a', 'r', 'b'), ('r', 'b', 'a'), ('b', 'a', 'r'), ('a', 'r', 'o'), ('r', 'o', 'u'), ('o', 'u', 's')]\n",
      "vein -> [('v', 'e', 'i'), ('e', 'i', 'n')]\n",
      "claim -> [('c', 'l', 'a'), ('l', 'a', 'i'), ('a', 'i', 'm')]\n",
      "detailed -> [('d', 'e', 't'), ('e', 't', 'a'), ('t', 'a', 'i'), ('a', 'i', 'l'), ('i', 'l', 'e'), ('l', 'e', 'd')]\n",
      "love -> [('l', 'o', 'v'), ('o', 'v', 'e')]\n",
      "zipper -> [('z', 'i', 'p'), ('i', 'p', 'p'), ('p', 'p', 'e'), ('p', 'e', 'r')]\n",
      "smoggy -> [('s', 'm', 'o'), ('m', 'o', 'g'), ('o', 'g', 'g'), ('g', 'g', 'y')]\n",
      "language -> [('l', 'a', 'n'), ('a', 'n', 'g'), ('n', 'g', 'u'), ('g', 'u', 'a'), ('u', 'a', 'g'), ('a', 'g', 'e')]\n",
      "protective -> [('p', 'r', 'o'), ('r', 'o', 't'), ('o', 't', 'e'), ('t', 'e', 'c'), ('e', 'c', 't'), ('c', 't', 'i'), ('t', 'i', 'v'), ('i', 'v', 'e')]\n",
      "rifle -> [('r', 'i', 'f'), ('i', 'f', 'l'), ('f', 'l', 'e')]\n",
      "coast -> [('c', 'o', 'a'), ('o', 'a', 's'), ('a', 's', 't')]\n",
      "guess -> [('g', 'u', 'e'), ('u', 'e', 's'), ('e', 's', 's')]\n",
      "spiky -> [('s', 'p', 'i'), ('p', 'i', 'k'), ('i', 'k', 'y')]\n",
      "multiply -> [('m', 'u', 'l'), ('u', 'l', 't'), ('l', 't', 'i'), ('t', 'i', 'p'), ('i', 'p', 'l'), ('p', 'l', 'y')]\n",
      "birth -> [('b', 'i', 'r'), ('i', 'r', 't'), ('r', 't', 'h')]\n",
      "railway -> [('r', 'a', 'i'), ('a', 'i', 'l'), ('i', 'l', 'w'), ('l', 'w', 'a'), ('w', 'a', 'y')]\n",
      "stiff -> [('s', 't', 'i'), ('t', 'i', 'f'), ('i', 'f', 'f')]\n",
      "discover -> [('d', 'i', 's'), ('i', 's', 'c'), ('s', 'c', 'o'), ('c', 'o', 'v'), ('o', 'v', 'e'), ('v', 'e', 'r')]\n",
      "book -> [('b', 'o', 'o'), ('o', 'o', 'k')]\n",
      "fall -> [('f', 'a', 'l'), ('a', 'l', 'l')]\n",
      "mountainous -> [('m', 'o', 'u'), ('o', 'u', 'n'), ('u', 'n', 't'), ('n', 't', 'a'), ('t', 'a', 'i'), ('a', 'i', 'n'), ('i', 'n', 'o'), ('n', 'o', 'u'), ('o', 'u', 's')]\n",
      "assorted -> [('a', 's', 's'), ('s', 's', 'o'), ('s', 'o', 'r'), ('o', 'r', 't'), ('r', 't', 'e'), ('t', 'e', 'd')]\n",
      "lyrical -> [('l', 'y', 'r'), ('y', 'r', 'i'), ('r', 'i', 'c'), ('i', 'c', 'a'), ('c', 'a', 'l')]\n",
      "jittery -> [('j', 'i', 't'), ('i', 't', 't'), ('t', 't', 'e'), ('t', 'e', 'r'), ('e', 'r', 'y')]\n",
      "murky -> [('m', 'u', 'r'), ('u', 'r', 'k'), ('r', 'k', 'y')]\n",
      "smelly -> [('s', 'm', 'e'), ('m', 'e', 'l'), ('e', 'l', 'l'), ('l', 'l', 'y')]\n",
      "ban -> [('b', 'a', 'n')]\n",
      "realize -> [('r', 'e', 'a'), ('e', 'a', 'l'), ('a', 'l', 'i'), ('l', 'i', 'z'), ('i', 'z', 'e')]\n",
      "rhetorical -> [('r', 'h', 'e'), ('h', 'e', 't'), ('e', 't', 'o'), ('t', 'o', 'r'), ('o', 'r', 'i'), ('r', 'i', 'c'), ('i', 'c', 'a'), ('c', 'a', 'l')]\n",
      "efficient -> [('e', 'f', 'f'), ('f', 'f', 'i'), ('f', 'i', 'c'), ('i', 'c', 'i'), ('c', 'i', 'e'), ('i', 'e', 'n'), ('e', 'n', 't')]\n",
      "prefer -> [('p', 'r', 'e'), ('r', 'e', 'f'), ('e', 'f', 'e'), ('f', 'e', 'r')]\n",
      "cows -> [('c', 'o', 'w'), ('o', 'w', 's')]\n",
      "cheerful -> [('c', 'h', 'e'), ('h', 'e', 'e'), ('e', 'e', 'r'), ('e', 'r', 'f'), ('r', 'f', 'u'), ('f', 'u', 'l')]\n",
      "toad -> [('t', 'o', 'a'), ('o', 'a', 'd')]\n",
      "deafening -> [('d', 'e', 'a'), ('e', 'a', 'f'), ('a', 'f', 'e'), ('f', 'e', 'n'), ('e', 'n', 'i'), ('n', 'i', 'n'), ('i', 'n', 'g')]\n",
      "stop -> [('s', 't', 'o'), ('t', 'o', 'p')]\n",
      "invent -> [('i', 'n', 'v'), ('n', 'v', 'e'), ('v', 'e', 'n'), ('e', 'n', 't')]\n",
      "cook -> [('c', 'o', 'o'), ('o', 'o', 'k')]\n",
      "interesting -> [('i', 'n', 't'), ('n', 't', 'e'), ('t', 'e', 'r'), ('e', 'r', 'e'), ('r', 'e', 's'), ('e', 's', 't'), ('s', 't', 'i'), ('t', 'i', 'n'), ('i', 'n', 'g')]\n",
      "useless -> [('u', 's', 'e'), ('s', 'e', 'l'), ('e', 'l', 'e'), ('l', 'e', 's'), ('e', 's', 's')]\n",
      "clear -> [('c', 'l', 'e'), ('l', 'e', 'a'), ('e', 'a', 'r')]\n",
      "jog -> [('j', 'o', 'g')]\n",
      "cheap -> [('c', 'h', 'e'), ('h', 'e', 'a'), ('e', 'a', 'p')]\n",
      "raise -> [('r', 'a', 'i'), ('a', 'i', 's'), ('i', 's', 'e')]\n",
      "avoid -> [('a', 'v', 'o'), ('v', 'o', 'i'), ('o', 'i', 'd')]\n",
      "increase -> [('i', 'n', 'c'), ('n', 'c', 'r'), ('c', 'r', 'e'), ('r', 'e', 'a'), ('e', 'a', 's'), ('a', 's', 'e')]\n",
      "noisy -> [('n', 'o', 'i'), ('o', 'i', 's'), ('i', 's', 'y')]\n",
      "serious -> [('s', 'e', 'r'), ('e', 'r', 'i'), ('r', 'i', 'o'), ('i', 'o', 'u'), ('o', 'u', 's')]\n",
      "sea -> [('s', 'e', 'a')]\n",
      "spiteful -> [('s', 'p', 'i'), ('p', 'i', 't'), ('i', 't', 'e'), ('t', 'e', 'f'), ('e', 'f', 'u'), ('f', 'u', 'l')]\n",
      "loud -> [('l', 'o', 'u'), ('o', 'u', 'd')]\n",
      "rod -> [('r', 'o', 'd')]\n",
      "replace -> [('r', 'e', 'p'), ('e', 'p', 'l'), ('p', 'l', 'a'), ('l', 'a', 'c'), ('a', 'c', 'e')]\n",
      "oranges -> [('o', 'r', 'a'), ('r', 'a', 'n'), ('a', 'n', 'g'), ('n', 'g', 'e'), ('g', 'e', 's')]\n",
      "border -> [('b', 'o', 'r'), ('o', 'r', 'd'), ('r', 'd', 'e'), ('d', 'e', 'r')]\n",
      "second -> [('s', 'e', 'c'), ('e', 'c', 'o'), ('c', 'o', 'n'), ('o', 'n', 'd')]\n",
      "cactus -> [('c', 'a', 'c'), ('a', 'c', 't'), ('c', 't', 'u'), ('t', 'u', 's')]\n",
      "perpetual -> [('p', 'e', 'r'), ('e', 'r', 'p'), ('r', 'p', 'e'), ('p', 'e', 't'), ('e', 't', 'u'), ('t', 'u', 'a'), ('u', 'a', 'l')]\n",
      "grip -> [('g', 'r', 'i'), ('r', 'i', 'p')]\n",
      "dispensable -> [('d', 'i', 's'), ('i', 's', 'p'), ('s', 'p', 'e'), ('p', 'e', 'n'), ('e', 'n', 's'), ('n', 's', 'a'), ('s', 'a', 'b'), ('a', 'b', 'l'), ('b', 'l', 'e')]\n",
      "sprout -> [('s', 'p', 'r'), ('p', 'r', 'o'), ('r', 'o', 'u'), ('o', 'u', 't')]\n",
      "pat -> [('p', 'a', 't')]\n",
      "friend -> [('f', 'r', 'i'), ('r', 'i', 'e'), ('i', 'e', 'n'), ('e', 'n', 'd')]\n",
      "boat -> [('b', 'o', 'a'), ('o', 'a', 't')]\n",
      "half -> [('h', 'a', 'l'), ('a', 'l', 'f')]\n",
      "elbow -> [('e', 'l', 'b'), ('l', 'b', 'o'), ('b', 'o', 'w')]\n",
      "poor -> [('p', 'o', 'o'), ('o', 'o', 'r')]\n",
      "beak -> [('b', 'e', 'a'), ('e', 'a', 'k')]\n",
      "busy -> [('b', 'u', 's'), ('u', 's', 'y')]\n",
      "monkey -> [('m', 'o', 'n'), ('o', 'n', 'k'), ('n', 'k', 'e'), ('k', 'e', 'y')]\n",
      "control -> [('c', 'o', 'n'), ('o', 'n', 't'), ('n', 't', 'r'), ('t', 'r', 'o'), ('r', 'o', 'l')]\n",
      "cautious -> [('c', 'a', 'u'), ('a', 'u', 't'), ('u', 't', 'i'), ('t', 'i', 'o'), ('i', 'o', 'u'), ('o', 'u', 's')]\n",
      "trace -> [('t', 'r', 'a'), ('r', 'a', 'c'), ('a', 'c', 'e')]\n",
      "deletion -> [('d', 'e', 'l'), ('e', 'l', 'e'), ('l', 'e', 't'), ('e', 't', 'i'), ('t', 'i', 'o'), ('i', 'o', 'n')]\n",
      "disarm -> [('d', 'i', 's'), ('i', 's', 'a'), ('s', 'a', 'r'), ('a', 'r', 'm')]\n",
      "november -> [('n', 'o', 'v'), ('o', 'v', 'e'), ('v', 'e', 'm'), ('e', 'm', 'b'), ('m', 'b', 'e'), ('b', 'e', 'r')]\n",
      "december -> [('d', 'e', 'c'), ('e', 'c', 'e'), ('c', 'e', 'm'), ('e', 'm', 'b'), ('m', 'b', 'e'), ('b', 'e', 'r')]\n",
      "horn -> [('h', 'o', 'r'), ('o', 'r', 'n')]\n",
      "cars -> [('c', 'a', 'r'), ('a', 'r', 's')]\n",
      "prose -> [('p', 'r', 'o'), ('r', 'o', 's'), ('o', 's', 'e')]\n"
     ]
    }
   ],
   "source": [
    "for i,j in d.items():\n",
    "    print(i,'->',j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(w1, w2, m, n):\n",
    "    if m == 0:\n",
    "        return n\n",
    "    if n == 0:\n",
    "        return m\n",
    "    if w1[m-1] == w2[n-1]:\n",
    "        return edit_distance(w1, w2, m-1, n-1)\n",
    "    return 1 + min(edit_distance(w1, w2, m, n-1),    # Insertion property follow\n",
    "                   edit_distance(w1, w2, m-1, n),    # deletion property follow\n",
    "                   edit_distance(w1, w2, m-1, n-1)    # substituion property follow\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    return intersection\n",
    "jaccard_similarity(d['november'],d['december'])  #Word taken by sir slide "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit Distance of lyrical and rhetorical is =5\n",
      "Edit Distance of november and december is =3\n"
     ]
    }
   ],
   "source": [
    "t=[]\n",
    "for i in d:\n",
    "    for j in d:\n",
    "        teja=jaccard_similarity(d[i],d[j])\n",
    "        if teja>=3 and i!=j and (i not in t and j not in t):\n",
    "            t.append(j)\n",
    "            t.append(i)\n",
    "            dist=edit_distance(i,j,len(i),len(j))\n",
    "            print(f'Edit Distance of {i} and {j} is ={dist}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THANK YOU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
